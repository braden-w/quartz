---
tags:
  - Topics/Artificial_Intelligence
  - Topics/Machine_Learning
  - Type/Courses
date: 2022-06-21
date modified: 2022-06-23
title: ML Safety Scholars Program
draft: false
---

# ML Safety Scholars Program
[[Week 2 Lab]]
- ## Introduction
	- [[We Need to Talk About A.I.]]
	- The main focus of machine learning is _making decisions or predictions based on data_.
	- [[Problem of Induction]]: Why do we think that previously seen data will help us predict the future?
- ## Problem Class
	- [[Supervised Learning]]
		- [[Classification]]
		- Regression
	- [[Unsupervised Learning]]
		- [[Density Estimation]]
		- Clustering
		- Dimensionality Reduction
- ## Evaluation Criteria
	- [[Loss Function]]
	- We want a [[Hypothesis]] with small loss (from [[Loss Function]]) on new data.
		- Proxy: Small loss on Training Data (which is a subset of all possible new data)
- ## Model Type
	- No Model - Sometimes, this is viable (like using )
	- Prediction Rule - More typical
		1. Fit a model to the training data
		2. Use the model to make predictions
- ## Learning Algorithms
- Example ways to create algorithms
	- Be a clever human
	- Use optimization methods (most frequent)
> Once we have described a class of models and a way of scoring a model given data, we have an algorithmic problem: what sequence of computational instructions should we run in order to find a good model from our class?

## The Classification Process
$$x\to\boxed{h}\to y$$

where $h$ is the hypothesis.

## Training Error
![](https://i.imgur.com/oj1C2ho.png)
where there are $n$ ' new examples.

### Training Error of Linear Classifier
- $\varepsilon_n(\theta, \theta_0)$
- If training data is [[Linearly Separable]], there must exist a model $\theta, \theta_0$ where [[Training Error]] is 0
	- This doesn't imply that there is a model where [[Testing Error]] is 0
### Testing Error of Linear Classifier
- $\varepsilon(\theta, \theta_0)$
- Testing error may not always be 0 

- Example of two points that are [[Linearly Separable]] but not [[Linearly Separable]] through the origin:
	- (1,1), +1
	- (2,2), -1

## Linear Classifiers
- [[Hyperplane]]
- [[Binary Classification]]
- [[Perceptron Algorithm]]
- How to transform dataset that is only linearly separable *with* offset to one that is [[Linearly Separable]] *without* offset
	- Add an extra dimension to all of the datapoints using the same (nonzero) number for each point
	- Add an extra dimension to all of the datapoints using a 1 for each point

References:
[Problem class | Introduction to ML | 6.036 Courseware | MIT Open Learning Library](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week1/intro_ml/?child=first)
https://phillipi.github.io/6.882/2020/notes/6.036_notes.pdf

## Week 2 Lab
# Evaluating learning methods

In HW 2, we will implement a very simple learning algorithm that, when given a set of possible hyperplanes and a data set $\mathcal{D}_{\it train}$ (consisting of a data array and label vector), returns the hyperplane that minimizes the number of errors on the training set. In the lecture, we have also considered the Perceptron algorithm (notes) and will implement it in homework this week. We will go on to study a number of other algorithms, all of which take in data as input and return a classification hypothesis as output.

In this lab, we will explore how to evaluate learning methods. You will have a checkoff conversation with a staff member at the end of the lab. After that, you're welcome to leave or stay to work on homework.

Note the following notation and definitions, used throughout this lab:

- A generator $\mathcal{G}$ is a function that takes as input nnn, the number of samples desired, and returns an $\mathcal{(X,y)}$ pair where $\mathcal{X}$ is a $\mathcal{d}$ by $\mathcal{n}$ array of randomly sampled data points and $\mathcal{y}$ is a 1 by $\mathcal{n}$ array of their corresponding labels {+1,−1}\{+1,−1\}{+1,−1}.

- A training dataset $\mathcal{D}_{\it train}$ is a set of labelled samples generated by $\mathcal{G}$, $\mathcal{X,y}$, where $x^{i}$ represents the features of an object to be classified (vector of real and/or discrete values), and yiy^{i}yi represents the label of $x^{i}$.

- A binary classifier $h$ is a function that takes an example $x \in R^d$ as input and returns +1+1+1 or −1−1−1 as output.

## 1) Evaluating a classifier

Imagine that you have a generator $\mathcal{G}$ that pulls from a finite dataset of millions of points.

Let's assume that $\mathcal{D}_{\it train}$​ is one such output of the generator $\mathcal{G}$.

Consider the situation in which you have run a machine learning algorithm on some training dataset $\mathcal{D}_{\it train}$, and it has returned to you a specific $h$. Your job is to design (but not implement yet!) a procedure for evaluating $h$'s effectiveness as a classifier. (Want more on classifiers? Check the notes)

Assume we have a score function that takes a classifier $h$, dataset $D$ - a tuple of data and labels: (X,y)(X,y)(X,y) - and returns the percentage of correctly classified examples as a decimal between 0 and 1. We'll package it as follows:

def eval_classifier(h, D):
    test_X, test_y = D
    return score(h, test_X, test_y)

A) Percy Eptron suggests reusing the training data to assess h:

eval_classifier(h, D_train)

Explain why Percy's strategy might not be so good.

> **Percy's strategy might not be so good because the training data might not be representative of the entire dataset or population, and thus the results of the evaluation might not be accurate.**

B) Now write down a better approach for evaluating $h$, which may use $h$, $\mathcal{G}$, and $\mathcal{D}_{\it train}$, and computes a score for $h$. The syntax is not important, but do write something down. What does this score measure and what is the range of possible outputs?

> **A better approach for evaluating $h$ would be to split the data into a training set and a test set. The classifier $h$ would be trained on the test set and evaluated on the test set.**
> 
C) Explain why your method might be more desirable than Percy's. What problem does it fix?

> **A Better approach for evaluating $h$ would be to split the data into a training set and a test set. The classifier $h$ would be trained on the test set and evaluated on the test set.**

D) How would your method from B score the classifier $h$, if $\mathcal{D}_{\it test}$ came from a different distribution than $\mathcal{G}$, but $\mathcal{D}_{\it train}$ was unchanged?

> **If $\mathcal{D}_{\it test}$ came from a different distribution than $\mathcal{G}$, but $\mathcal{D}_{\it train}$ was unchanged, then the method from B would score the classifier $h$ by training on $\mathcal{D}_{\it train}$ and testing on $\mathcal{D}_{\it test}$.**
## 2) Evaluating a learning algorithm

A learning algorithm is a function $L$ that takes as input

    data set $\mathcal{D}_{\it train}$ as training data

and returns

    a classifier $h$.

A) Would running the learning algorithm $L$ on two different training datasets Dtrain1\mathcal{D}_{\it train_1}Dtrain1​​ and Dtrain2\mathcal{D}_{\it train_2}Dtrain2​​ produce the same classifier? In other words, would h1h_1h1​ = L(Dtrain1)L(\mathcal{D}_{\it train_1})L(Dtrain1​​) be the same classifier as h2h_2h2​ = L(Dtrain2)L(\mathcal{D}_{\it train_2})L(Dtrain2​​)? What if those training datasets were pulled from the same distribution?

Now, consider a situation in which someone is trying to sell you a new learning algorithm, and you want to know how good it is. There is an interesting result that says that without any assumptions about your data, There is no learning algorithm that, for all data sources, is better than every other learning algorithm. So, you'll need to assess the learning algorithm's performance in the context of a particular data source.

Check Yourself: What is the difference between a classifier and a learning algorithm? Understanding the distinction will help you when thinking about this question. (Stuck? Check the notes)

Assume that you have a generator of labeled data, $\mathcal{G}$, which will be suitable for your application. The learning algorithm's performance on $\mathcal{G}$-generated data will be a good predictor of the learning algorithm's performance on data from your application. (You can review how to evaluate learning algorithms in the notes)

B) Linnea Separatorix wants to evaluate a learning algorithm, and suggests the following procedure:

def eval_learning_alg(L, G, n):
    # draw a set of n training examples (points and labels)
    train_X, train_y = G(n)
    # run L
    h = L(train_X, train_y)
    # evaluate using your classifier scoring procedure, on some new labeled data
    test_data = G(n) # draw new set of test data
    return eval_classifier(h, test_data)

Check Yourself: What are GGG and nnn in the code above?

Explain why Linnea's strategy might not be so good.

C) Next, Linnea decides to generate one classifier $h$ but evaluate that classifier with multiple (10) test sets in her eval_learning_alg. More specifically, Linnea changed her code above into:

def eval_learning_alg(L, G, n):
    # draw a set of n training examples (points and labels)
    train_X, train_y = G(n)
    # run L
    h = L(train_X, train_y)
    # evaluate using your classifier scoring procedure, on some new labeled data
    score = 0
    for i in range(10):
        test_data = G(n) # draw new set of test data
        score += eval_classifier(h, test_data)
    return score/10

Is Linnea's strategy good now? Explain why or why not.

Check Yourself: How many classifiers is Linea generating and testing from the learning algorithm?

D) Now design a better procedure for evaluating $L$. Write pseudocode for a procedure that takes $L$, $\mathcal{G}$ and nnn and returns a score. Say what the output score measures and what the best and worst values are.

def better_eval_learning_alg(L, G, n):
    # your procedure

E) Explain why your method might be more desirable than Linnea's.

## 3) Evaluating a learning algorithm with a small amount of data

In reality, it's almost never possible to have a generator of all the data you want; in fact, in some domains data is very expensive to collect, and so you are given a fixed, small set of samples. Now assume that you only have 100 labeled data points to use for training and testing/evaluation.

A) In the last section, you thought about how to evaluate a learning algorithm. Now that you are given only 100 labeled data points in total, how would you evaluate a learning algorithm? Specifically, how would you implement better_eval_learning_alg from 2C) without $\mathcal{G}$ but instead with your 100 labeled data? (You don't need to write out new pseudocode for better_eval_learning_alg but still think about how you would implement it.)
