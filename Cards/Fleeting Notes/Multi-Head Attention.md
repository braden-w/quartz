---
tags:
 - On/Machine_Learning
 - Type/Concept
title: Multi-Head Attention
date: "2022-07-30"
date modified: "2022-07-30"
---

# Multi-Head Attention
Type of attention mechanism that allows a model to attend to multiple pieces of information at the same time. This can be useful in tasks where multiple pieces of information need to be considered, such as machine translation or question answering.

See [[Attention Is All You Need]].
