<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>On/Machine_Learning on</title><link>https://braden-w.github.io/quartz/tags/On/Machine_Learning/</link><description>Recent content in On/Machine_Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://braden-w.github.io/quartz/tags/On/Machine_Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Attention/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Attention/</guid><description>Attention See [[Attention Is All You Need|Transformers]].
Inspiration Use new context vector at each step of decoder! Use an encoder and decoder Procedure Intuition: Context vector attends to the relevant part of the input sequence</description></item><item><title>Megatron Equivalent Costs $430,000 on Amazon AWS</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Megatron-Equivalent-Costs-430000-on-Amazon-AWS/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Megatron-Equivalent-Costs-430000-on-Amazon-AWS/</guid><description>Megatron Equivalent Costs $430,000 on Amazon AWS Nvidia trained over 9 days on 512 GPUs.</description></item><item><title>Reinforcement Learning</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Reinforcement-Learning/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Reinforcement-Learning/</guid><description>Reinforcement Learning Create an agent that performs actions in environment, and receives rewards. Goal: learn how to take actions that maximize reward.</description></item><item><title>[[Deep Learning]] For NLP</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Deep-Learning-For-NLP/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Deep-Learning-For-NLP/</guid><description>[[Deep Learning]] For NLP Lots of progress recently People trust [[Wikipedia]], [[Google Translate]], etc. more than other people, in some cases Language models assign probability to text $p(x_0, …, x_n)$ Most popular method is to factorize distribution using chain rule: $$p(x_0,…x_n) = p(x_0)p(x_1 \mid x_0) \cdots p(x_n \mid x_{n-1})$$ Neural Language Models [[Convolutional Language Models]] (CLMs) Earliest</description></item><item><title>BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding - Abstract</title><link>https://braden-w.github.io/quartz/obsidian/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-Abstract/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-Abstract/</guid><description>BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding - Abstract This research paper discusses the pre-training of deep bidirectional transformers for language understanding using the BERT model.</description></item><item><title>Deep Residual Learning for Image Recognition</title><link>https://braden-w.github.io/quartz/obsidian/Deep-Residual-Learning-for-Image-Recognition/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Deep-Residual-Learning-for-Image-Recognition/</guid><description>Deep Residual Learning for Image Recognition This paper presents a residual learning framework to ease the training of deep neural networks.</description></item><item><title>Evaluating Language Models is Easy, but Evaluating Generated Text is Hard</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Evaluating-Language-Models-is-Easy-but-Evaluating-Generated-Text-is-Hard/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Evaluating-Language-Models-is-Easy-but-Evaluating-Generated-Text-is-Hard/</guid><description>Evaluating Language Models is Easy, but Evaluating Generated Text is Hard</description></item><item><title>How to ML Paper Replication</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/How-to-ML-Paper-Replication/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/How-to-ML-Paper-Replication/</guid><description>How to ML Paper Replication References (References:: Quick collation of resources about ML paper replication - Google Docs)</description></item><item><title>Multi-Head Attention</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Multi-Head-Attention/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Multi-Head-Attention/</guid><description>Multi-Head Attention Type of attention mechanism that allows a model to attend to multiple pieces of information at the same time.</description></item><item><title>Support Vector Machine (SVM)</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Support-Vector-Machine-SVM/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Support-Vector-Machine-SVM/</guid><description>Support Vector Machine (SVM)</description></item><item><title>Advanced, Planning, Strategically Aware Systems</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Advanced-Planning-Strategically-Aware-Systems/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Advanced-Planning-Strategically-Aware-Systems/</guid><description>Advanced, Planning, Strategically Aware Systems</description></item><item><title>Analytical Gradient</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Analytical-Gradient/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Analytical-Gradient/</guid><description>Analytical Gradient Derive the actual [[Gradient]], usually using [[Back-Propagation]] or [[Calculus]], then apply.
This is more exact and faster than [[Numeric Gradient]], but error-prone.</description></item><item><title>Nesterov Momentum</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Nesterov-Momentum/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Nesterov-Momentum/</guid><description>Nesterov Momentum</description></item><item><title>Numeric Gradient</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Numeric-Gradient/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Numeric-Gradient/</guid><description>Numeric Gradient The use of adding very small $h$ to appoximate gradient and then use [[Gradient Descent]]. Is approximate and slow (not a good idea), but useful to think about.</description></item><item><title>ML Safety Scholars Program</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/ML-Safety-Scholars-Program/</link><pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/ML-Safety-Scholars-Program/</guid><description>ML Safety Scholars Program [[Introduction to Machine Learning (MIT)]] [[Deep Learning for Computer Vision (UMich)]] [[Week 2 Lab]]
[[2022-06-22]] Week 1 Readings [[We Need to Talk About A.</description></item><item><title>Deep Learning</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Deep-Learning/</link><pubDate>Sat, 16 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Deep-Learning/</guid><description>Deep Learning [[Symbolic Learning Vs. Deep Learning]]
How Deep Learning Got Popular Data Size Increase in Compute Deep Learning entered Imagenet in 2012, and the competition hasn&amp;rsquo;t been the same ever since, quickly surpassing human level image recognition</description></item><item><title>Natural Language Processing</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Natural-Language-Processing/</link><pubDate>Sat, 16 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Natural-Language-Processing/</guid><description>Natural Language Processing</description></item><item><title>Dropout - A Simple Way to Prevent Neural Networks From Overfitting</title><link>https://braden-w.github.io/quartz/obsidian/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-From-Overfitting/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-From-Overfitting/</guid><description>Dropout - A Simple Way to Prevent Neural Networks From Overfitting This research paper explores the idea of using dropout to prevent overfitting in deep neural networks.</description></item><item><title>Estimation Error</title><link>https://braden-w.github.io/quartz/obsidian/Estimation-Error/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Estimation-Error/</guid><description>Estimation Error When the hypothesis class cannot represent a hypothesis that performs well on the test data
References (References:: https://openlearninglibrary.</description></item><item><title>Structural Error</title><link>https://braden-w.github.io/quartz/obsidian/Structural-Error/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Structural-Error/</guid><description>Structural Error When the parameters of a hypothesis cannot be estimated well based on the training data
https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week5/week5_homework/?child=first</description></item><item><title>Introduction to Machine Learning (MIT)</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/Introduction-to-Machine-Learning-MIT/</link><pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/Introduction-to-Machine-Learning-MIT/</guid><description>Introduction to Machine Learning (MIT) Introduction The main focus of machine learning is making decisions or predictions based on data. [[Problem of Induction]]: Why do we think that previously seen data will help us predict the future?</description></item><item><title>Feature Transformations</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Feature-Transformations/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Feature-Transformations/</guid><description>Feature Transformations A mathematical transformation in which we apply a formula to make data easier to process.
References (References:: Problem class | Introduction to ML | 6.</description></item><item><title>Classification</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Classification/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Classification/</guid><description>Classification References (References:: Classification | Linear classifiers | 6.036 Courseware | MIT Open Learning Library)</description></item><item><title>Density Estimation</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Density-Estimation/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Density-Estimation/</guid><description>Density Estimation Predict probability $Pr(x^{(n+1)})$ of an element drawn from distribution $Pr(X)$
References:</description></item><item><title>Hyperplane</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Hyperplane/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Hyperplane/</guid><description>Hyperplane Hyperplane defined by a norm vector $\theta$ is a set of points $x$ such that $\theta \cdot x = 0$.</description></item><item><title>Loss Function</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Loss-Function/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Courses/ML-Safety-Scholars-Program/Loss-Function/</guid><description>Loss Function How sad are we when we predicted $g$ when answer actually $a$? $$L(g,a)$$
where guess $g\in {+1, -1}$ and actual $a \in {+1, -1}$.</description></item><item><title>Set Realistic Expectations in Your Machine Learning Journey</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Videos/Set-Realistic-Expectations-in-Your-Machine-Learning-Journey/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Videos/Set-Realistic-Expectations-in-Your-Machine-Learning-Journey/</guid><description>Set Realistic Expectations in Your Machine Learning Journey [[Entry Level Machine Learning is Actually Data Analyst. ML Engineer Requires 3-5 Years of Experience]] [[Most Projects Are Not Deep Learning, but Classification Regression on Structured Data.</description></item><item><title>The Top Model for All Classification Regression Problems is Gradient Boosters (and Most Likely What You'll Be Working with).</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/The-Top-Model-for-All-Classification-Regression-Problems-is-Gradient-Boosters-and-Most-Likely-What-Youll-Be-Working-with./</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/The-Top-Model-for-All-Classification-Regression-Problems-is-Gradient-Boosters-and-Most-Likely-What-Youll-Be-Working-with./</guid><description>The Top Model for All Classification Regression Problems is Gradient Boosters (and Most Likely What You&amp;rsquo;ll Be Working with). References (References:: https://www.</description></item><item><title>There is ZERO Bias in Machine Learning Models</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Videos/There-is-ZERO-Bias-in-Machine-Learning-Models/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Videos/There-is-ZERO-Bias-in-Machine-Learning-Models/</guid><description>There is ZERO Bias in Machine Learning Models When people talk about ethical AI, they are talking about data, not intrinsic ethics of an AI model.</description></item><item><title>Why Being a Data Engineer is Better Than Machine Learning Engineer</title><link>https://braden-w.github.io/quartz/obsidian/Sources/Videos/Why-I-Prefer-Data-Engineering-Roles/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Sources/Videos/Why-I-Prefer-Data-Engineering-Roles/</guid><description>Why I Prefer Data Engineering Roles Data engineers have more autonomy, less meetings, and less burden than machine learning engineering. Machine Learning engineer is overhyped.</description></item></channel></rss>