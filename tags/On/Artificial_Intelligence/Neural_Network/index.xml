<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>On/Artificial_Intelligence/Neural_Network on</title><link>https://braden-w.github.io/quartz/tags/On/Artificial_Intelligence/Neural_Network/</link><description>Recent content in On/Artificial_Intelligence/Neural_Network on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://braden-w.github.io/quartz/tags/On/Artificial_Intelligence/Neural_Network/index.xml" rel="self" type="application/rss+xml"/><item><title>Your Brain is Literally a Neural Network. Your Attitude Towards the World Can Change Imperceptibly, Ever-so-slightly Changing Weights When Accepting New Training Data, Even if the Original Input is Gone.</title><link>https://braden-w.github.io/quartz/obsidian/Your-Brain-is-Literally-a-Neural-Network.-Your-Attitude-Towards-the-World-Can-Change-Imperceptibly-Ever-so-slightly-Changing-Weights-When-Accepting-New-Training-Data-Even-if-the-Original-Input-is-Gone./</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Your-Brain-is-Literally-a-Neural-Network.-Your-Attitude-Towards-the-World-Can-Change-Imperceptibly-Ever-so-slightly-Changing-Weights-When-Accepting-New-Training-Data-Even-if-the-Original-Input-is-Gone./</guid><description>Your Brain is Literally a Neural Network. Your Attitude Towards the World Can Change Imperceptibly, Ever-so-slightly Changing Weights When Accepting New Training Data, Even if the Original Input is Gone.</description></item><item><title>Batch Normalization</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Batch-Normalization/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Batch-Normalization/</guid><description>Batch Normalization Normalizing activation vectors from hidden layers**Â using the first and the second statistical moments (mean and variance) of the current batch.</description></item><item><title>Convolutional Language Models</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Language-Models/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Language-Models/</guid><description>Convolutional Language Models</description></item><item><title>Convolutional Neural Networks</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Neural-Networks/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Neural-Networks/</guid><description>Convolutional Neural Networks See [[Deep Neural Networks]].</description></item><item><title>Layer Normalization</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Layer-Normalization/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Layer-Normalization/</guid><description>Layer Normalization Improvements over [[Batch Normalization]]
Can deal with sequences Any batch number sworks Can paralllize Better for RNNs Paper Summary This research paper explores the new technique of layer normalization.</description></item><item><title>Convolution Layer</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolution-Layer/</link><pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolution-Layer/</guid><description>Convolution Layer References (References:: Lecture 7: Convolutional Networks - YouTube)</description></item><item><title>Convolutional Networks</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Networks/</link><pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Networks/</guid><description>Convolutional Networks Goal: Respect spatial structure of input. Transition from fully connected [[Neural Network]]s to CNNs.
[[Convolution Layer]]
References (References:: Lecture 7: Convolutional Networks - YouTube)</description></item><item><title>99% Of Everything is Done with a [[Neural Network]]</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/99-Of-Everything-is-Done-with-a-Neural-Network/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/99-Of-Everything-is-Done-with-a-Neural-Network/</guid><description>99% Of Everything is Done with a [[Neural Network]] Neural Networks are everywhere. Everything that is cutting edge is likely a Neural Network.</description></item><item><title>[[Neural Network Node]]'s Output</title><link>https://braden-w.github.io/quartz/obsidian/Neural-Network-Nodes-Output/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Neural-Network-Nodes-Output/</guid><description>[[Neural Network Node]]&amp;rsquo;s Output Each neuron or node has [[Weight (W)]] $W$ and [[Bias (B)]] $B$.
A formula $$W \times X + B$$ and is then passed into an [[Activation Function]].</description></item><item><title>[[Neural Network]] Node</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Neural-Network-Node/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Neural-Network-Node/</guid><description>[[Neural Network]] Node Each [[Neural Network Node|Node]] has a [[Weight (W)]] $W$ and [[Bias (B)]] $B$. It has an output of</description></item><item><title>Activation Function</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Activation-Function/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Activation-Function/</guid><description>Activation Function Every [[Neural Network Node]] has one, so each layer has many.
We pass the [[Neural Network Node&amp;rsquo;s Output]] into this activation function to get a result in the forward step.</description></item><item><title>Bias (B)</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Bias-B/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Bias-B/</guid><description>Bias (B) $B$</description></item><item><title>ReLU</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/ReLU/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/ReLU/</guid><description>ReLU Note that the blue line is the gradient.
Advantages Even if the model or neuron has an enormous weight on the output being pushed out, the gradient is constant.</description></item><item><title>Sigmoid Function</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Sigmoid-Function/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Sigmoid-Function/</guid><description>Sigmoid Function A type of [[Activation Function]]
Notice how as the function increases, its derivative decreases. This is a problem that causes the [[Vanishing Gradient Problem]].</description></item><item><title>Softmax</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Softmax/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Softmax/</guid><description>Softmax</description></item><item><title>Softmax Loss</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Softmax-Loss/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Softmax-Loss/</guid><description>Softmax Loss Equals [[Softmax]], which is an [[Activation Function]], + Cross-Entropy Loss, which is sum of the negative logarithm of the probabilities</description></item><item><title>Vanishing Gradient Problem</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Vanishing-Gradient-Problem/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Vanishing-Gradient-Problem/</guid><description>Vanishing Gradient Problem The problem is that during [[Backpropagation]], the gradients will sharply decrease over time.
The [[Weight (W)|Weights]] of a node will not be updated even if there is a high loss, because the [[Gradient]] is zero from [[Backpropagation]].</description></item><item><title>Weight (W)</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Weight-W/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Weight-W/</guid><description>Weight (W)</description></item><item><title>Backpropagation</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Backpropagation/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Backpropagation/</guid><description>Backpropagation The update step for a [[Neural Network]], in which we calculate the [[Gradient]] of each node by differentiating the node&amp;rsquo;s [[Activation Function]].</description></item><item><title>Connecting the [[Neuron]] to the [[Activation Function]]</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Connecting-the-Neuron-to-the-Activation-Function/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Connecting-the-Neuron-to-the-Activation-Function/</guid><description>Connecting the [[Neural Network Node]] to the [[Activation Function]] Don&amp;rsquo;t compare [[Neural Network Node|Neurons]] too much to [[Neural Network|Neural Networks]], it&amp;rsquo;s more of a legacy connection.</description></item><item><title>Why We Need an [[Activation Function]] in a [[Neural Network]]</title><link>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Why-We-Need-Activation-Function-in-a-Neural-Network/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://braden-w.github.io/quartz/obsidian/Cards/Fleeting-Notes/Why-We-Need-Activation-Function-in-a-Neural-Network/</guid><description>Why We Need Activation Function in a Neural Network If we don&amp;rsquo;t have an activation function, then the [[Neural Network]] ends up just being a [[Linear Classifier]].</description></item></channel></rss>