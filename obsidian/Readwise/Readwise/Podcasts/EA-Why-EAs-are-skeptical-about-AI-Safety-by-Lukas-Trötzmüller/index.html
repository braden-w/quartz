<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="EA - Why EAs are skeptical about AI Safety by Lukas Tr√∂tzm√ºller  Metadata  Author: [[The Nonlinear Library]] Full Title: EA - Why EAs are skeptical about AI Safety by Lukas Tr√∂tzm√ºller Category: #podcasts URL: https://share."><title>üßô‚Äç‚ôÇÔ∏èOptim</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://braden-w.github.io/quartz//icon.png><link href=https://braden-w.github.io/quartz/styles.c605bbadbdae7a3dff05f2f1a934772a.min.css rel=stylesheet><link href=https://braden-w.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://braden-w.github.io/quartz/js/darkmode.88ed88da9a3165d431ef1df4e618aad8.min.js></script>
<script src=https://braden-w.github.io/quartz/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://braden-w.github.io/quartz/js/popover.53ad9a087e3feeaaa12b63bfd02d923b.min.js></script>
<script src=https://braden-w.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://braden-w.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script>const BASE_URL="https://braden-w.github.io/quartz/",fetchData=Promise.all([fetch("https://braden-w.github.io/quartz/indices/linkIndex.e680114dd38aa7d9c0fd547851b90658.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://braden-w.github.io/quartz/indices/contentIndex.43d89575e3ce07ec985476cd35e7f14b.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://braden-w.github.io/quartz",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://braden-w.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!0,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!0,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'‚Äô':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/braden-w.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label placeholder><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://braden-w.github.io/quartz/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://braden-w.github.io/quartz/>üßô‚Äç‚ôÇÔ∏èOptim</a></h1><div class=spacer></div><div id=search-icon><p></p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title"/><desc id="desc"/><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title/><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title/><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Unknown
<a href=https://github.com/braden-w/quartz/tree/hugo/content/obsidian/Readwise/Readwise/Podcasts/EA%20-%20Why%20EAs%20are%20skeptical%20about%20AI%20Safety%20by%20Lukas%20Tr%c3%b6tzm%c3%bcller.md rel=noopener></a></p><ul class=tags></ul><aside class=mainTOC><details><summary></summary><nav id=TableOfContents><ol><li><a href=#metadata>Metadata</a></li><li><a href=#highlights>Highlights</a></li></ol></nav></details></aside><a href=#ea---why-eas-are-skeptical-about-ai-safety-by-lukas-tr√∂tzm√ºller><h1 id=ea---why-eas-are-skeptical-about-ai-safety-by-lukas-tr√∂tzm√ºller><span class=hanchor arialabel=Anchor># </span>EA - Why EAs are skeptical about AI Safety by Lukas Tr√∂tzm√ºller</h1></a><p><img src="https://images.weserv.nl/?url=https%3A%2F%2Fspeechkit-prod.s3.eu-west-1.amazonaws.com%2Fdistribution_images%252Fdistribution%252F%252FNonlinearnormal.png&w=100&h=100" width=auto alt=rw-book-cover></p><a href=#metadata><h2 id=metadata><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">The Nonlinear Library</a></li><li>Full Title: EA - Why EAs are skeptical about AI Safety by Lukas Tr√∂tzm√ºller</li><li>Category: #podcasts</li><li>URL:
<a href=https://share.snipd.com/episode/b8998e2b-2267-4c73-bdb1-7171d4c17977 rel=noopener>https://share.snipd.com/episode/b8998e2b-2267-4c73-bdb1-7171d4c17977</a></li></ul><a href=#highlights><h2 id=highlights><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>Ai Development Is Limited by Neuro Science
Summary:
I don&rsquo;t see how you could suddenly have an instance to destroy humanity, accidentally or otherwise. The world is just way too complicated. An ai Couldn&rsquo;t just influence it easily. I don&rsquo;t think superintellience will be all that powerful. Outcomes in the real world are dominated by fundamentally unpredictable factors which places limits on the utility of intelligence. A g i development is limited by neuro science. We need to first understand human intelligence before we build a g i.
Transcript:
Speaker 1
Great intelligence does not translate into great influence. The world is just so complicated. If you look at a weather model that predicts just the weather, which is much more simple than the world in total, to somewhat ted a day ahead or two, that&rsquo;s possible. But further ahead it often fails. It&rsquo;s a first order chaotic system. But the world around us is a second order chaotic system. I don&rsquo;t see how you could suddenly have an instance to destroy humanity, accidentally or otherwise. The world is just way too complicated. An ai. Couldn&rsquo;t just influence it easily. I don&rsquo;t think superintellience will be all that powerful. Outcomes in the real world are dominated by fundamentally unpredictable factors, which places limits on the utility of intelligence. A g i development is limited by neuro science. The speed of a a development so far does not warrant even human level a g i any time soon. What really underscores that is there is a rate limiting effect of nero science and cognitive science. We need to first understand human intelligence before we build a g i. A g i will not come before we have a good understanding of what makes the brain work. In terms of how we understand intelligence, it is all biological, and we don&rsquo;t have any idea how it really works. We have a semblance of an idea. But when we are working with a i, we are working in silican and it is all digital. Whereas biological intelligence is more like stacastic and random and probablistic. There are millions of impulses feeding into billions of nurons, and it somehow makes up a person or a thought. And we are so far away from understanding is actually going on. The amount of processing in silica needed to process a single second of brain activity is insane. (
<a href=https://share.snipd.com/snip/0dc7a8f4-059b-4fdd-952b-a4c74d0db794 rel=noopener>Time¬†0:11:23</a>)</li><li>A i Safety Is Like The Perfect Mind Vulture
Summary:
The ai safety community looks like a cult around aliser udkowski. He is deliberately cultivating an bove everything mistike. The discussion seems too unipolar, too much focused on arguments from a leser. A i safety seams like the perfect mind virus for people in tech. How much are you willing to trust a i researchers saying that their own problem is the most important one? My problem will cause total human extinction. I wonder if that results in biasing people to place a lot of emphasis on it possible solution. Maybe putting more emphasis on adjacent areas like explainability, there might be more opportunities there.
Transcript:
Speaker 1
Of course, these wouldn&rsquo;t do any more good if they did another topic, because this is the topic they are expert on. So their resources are in the right place. And if other people want to go into ai safety too, then i say go for it, because then they will be working from their strengths. We are already on a good path. I agree it is important to invest a lot of resources into a i safety, but i believe that the current resources, plus the ones we expect to be invested in the future, are probably enough to fix the problem. Concerns about community epistemics and ideology. The a i safety community looks like a cult around aliser udkowski. I don&rsquo;t trust it because it looks similar to cults i&rsquo;ve seen. He is deliberately cultivating an bove everything mistike. The discussion seems too unipolar, too much focused on arguments from a leser. A i safety seams like the perfect mind virus for people in tech. The thing that i work on every day already that is the most important thing ever in history. How much are you willing to trust a i researchers saying that their own problem is the most important one. My problem will cause total human extinction. I think there could possibly be a bit of when you start working on something that becomes your whole world, you start thinking about the issues and problems, and you can at some point lose objectivity you are so integrated into fixing that problem or issue. For example, medical students often think they have some of the eases they are studying. It might become more real because you are thinking about it all the time. I wonder if that results in biasing people to place a lot of emphasis on it possible solution. Maybe putting more emphasis on adjacent areas like explainability, there might be more opportunities there. (
<a href=https://share.snipd.com/snip/68320713-f8ec-433a-890a-4f9c42716144 rel=noopener>Time¬†0:27:29</a>)</li><li>Is this an Existential Risk?
Summary:
I believe this a i issue is becoming important within a because we are all driven by our emotions. I sense that is why everyone is moved to work on this problem. It seems like a great thing to work on. And then everyone uses their intelligence to justify what they are doing, which is to work on problem. So terese a lot of effort going on to demonstrate that this is an existential risk.
Transcript:
Speaker 1
How much are you willing to trust a i researchers saying that their own problem is the most important one. My problem will cause total human extinction. I think there could possibly be a bit of when you start working on something that becomes your whole world, you start thinking about the issues and problems, and you can at some point lose objectivity you are so integrated into fixing that problem or issue. For example, medical students often think they have some of the eases they are studying. It might become more real because you are thinking about it all the time. I wonder if that results in biasing people to place a lot of emphasis on it possible solution. Maybe putting more emphasis on adjacent areas like explainability, there might be more opportunities there. I believe this a i issue is becoming important within a because we are all driven by our emotions. It feels plausible that we get wiped out fear. And also, you get a lot of status by working on this problem. I sense that is why everyone is moved to work on this problem. It seems like a great thing to work on. And then everyone uses their intelligence to justify what they are doing, which is to work on problem, because what else do you work on? It&rsquo;s a really cool problem to work on. So terese a lot of effort going on to demonstrate that this is an existential risk, because of natural human motivations. I can understand how this community has come to work on this, because it feels like an important issue. And if you stopped to work on this, then what else would you work on? The reason everyone is working on it is that it&rsquo;s a very human thing, caused by human emotions. Interviewer, would you call that motivated reasoning? Yes. Everyone is motivated to get prestige and status on what seems like a very important problem. (
<a href=https://share.snipd.com/snip/daf24dcd-c88d-46ac-82ed-9e09c39c1692 rel=noopener>Time¬†0:28:21</a>)</li><li>Is this an Existential Risk?
Summary:
Everyone is motivated to get prestige and status on what seems like a very important problem. Once you have invested all that time an effort, you don&rsquo;t want to be in a position to admit it&rsquo;s not such a serious problem. I am especially worried about a lot of people who strongly believing that this is the most likely existential risk because i don&rsquo;t understand where this intuition comes from. It feels plausible that we get wiped out fear. And also, you get a lot of status by working on this problem.I can understand how this community has come to work on this, because it feels like an important issue. And then everyone uses their intelligence to justify what they are doing, which is to
Transcript:
Speaker 1
It feels plausible that we get wiped out fear. And also, you get a lot of status by working on this problem. I sense that is why everyone is moved to work on this problem. It seems like a great thing to work on. And then everyone uses their intelligence to justify what they are doing, which is to work on problem, because what else do you work on? It&rsquo;s a really cool problem to work on. So terese a lot of effort going on to demonstrate that this is an existential risk, because of natural human motivations. I can understand how this community has come to work on this, because it feels like an important issue. And if you stopped to work on this, then what else would you work on? The reason everyone is working on it is that it&rsquo;s a very human thing, caused by human emotions. Interviewer, would you call that motivated reasoning? Yes. Everyone is motivated to get prestige and status on what seems like a very important problem. And once you have invested all that time an effort, you don&rsquo;t want to be in a position to admit it&rsquo;s not such a serious problemi understand how people are very convinced about this likelihood of a i risk. I don&rsquo;t understand how this can even be talked about, or discussed about in such detail that you can even quatify some amount of risk. I am especially worried about a lot of eas very strongly believing that this is the most likely existential risk, because i don&rsquo;t understand where this intuition comes from. I am worried that this belief is traceable back to a few thinkers that e a respect a lot, like nick bostrum, alizer, dkowski and some others. It looks to me that a lot of arguments get traced to them, and people who are not involved in the e a community do not seem to come up with the same beliefs by themselves. (
<a href=https://share.snipd.com/snip/4497877d-2feb-4f31-86ec-ba02d1a92a96 rel=noopener>Time¬†0:29:06</a>)</li><li>A g i
Summary:
There is no tangible evidence i have seen that a g i is possible to create, or even that in our present day world there is evidence for its possibility. This comes perhaps from a lack of communication or data from people who are doing this research. I heard lots of big arguments, but i want more specifics, which a i things already went wrong. What unexpected results did people get with gpt three and ale? I would also like to hear more about deep brain neuroscience.
Transcript:
Speaker 1
At the same time, i do believe some of the catastrophe scenarios, but i also have a big emotional thing telling me that maybe people will do the right thing and we don&rsquo;t need to invest that much resources into safety. This comes perhaps from a lack of communication or data from people who are doing this research. I heard lots of big arguments, but i want more specifics, which a i things already went wrong. What unexpected results did people get with gpt three and ale? I would also like to hear more about deep brain i wonder if i have a basic misunderstanding of e a. Because, as it sounds from an outsider point of view, if it is e a, in trying to find good opportunities for philanthropy to have the most effect, then it does seem like focus on a i safety is overrated. There is no tangible evidence i have seen that a g i is possible to create, or even that in our present day world there is evidence for its possibility. I haven&rsquo;t seen any examples that lead me to think that it could happen. If somebody showed me a nice, pierr reviewed research paper and trials that said, o y, we have this preliminary model for an a g i, then i would be a little bit more spooked. But it&rsquo;s something i have never seen. But i do see people starving. The risk is too small. In the framework of e a, you want to work on something that you know you can have an impact on, given infinite resources, one should work on a g i. Safety, definitely. But given finite resources, if you don&rsquo;t have sufficient knowledge what the risk is, and in this case, the risk is vanishinglya, in that case it is quite difficult to justify thinking about it as much as people currently are. Small research institutes are unlikely to have an impact. (
<a href=https://share.snipd.com/snip/bf58a5c8-a15e-4fec-adf3-e81512478fba rel=noopener>Time¬†0:33:59</a>)</li><li>A g i
Summary:
There is no tangible evidence i have seen that a g i is possible to create, or even that in our present day world there is evidence for its possibility. This comes perhaps from a lack of communication or data from people who are doing this research. I would also like to hear more about deep brain i wonder if i have a basic misunderstanding of e a. If you want to work on something that you know you can have an impact on, given infinite resources, one should work on a g i. But given finite resources, if you don&rsquo;t have sufficient knowledge what the risk is, it&rsquo;s quite difficult to justify thinking about it as much as people currently are.
Transcript:
Speaker 1
This comes perhaps from a lack of communication or data from people who are doing this research. I heard lots of big arguments, but i want more specifics, which a i things already went wrong. What unexpected results did people get with gpt three and ale? I would also like to hear more about deep brain i wonder if i have a basic misunderstanding of e a. Because, as it sounds from an outsider point of view, if it is e a, in trying to find good opportunities for philanthropy to have the most effect, then it does seem like focus on a i safety is overrated. There is no tangible evidence i have seen that a g i is possible to create, or even that in our present day world there is evidence for its possibility. I haven&rsquo;t seen any examples that lead me to think that it could happen. If somebody showed me a nice, pierr reviewed research paper and trials that said, o y, we have this preliminary model for an a g i, then i would be a little bit more spooked. But it&rsquo;s something i have never seen. But i do see people starving. The risk is too small. In the framework of e a, you want to work on something that you know you can have an impact on, given infinite resources, one should work on a g i. Safety, definitely. But given finite resources, if you don&rsquo;t have sufficient knowledge what the risk is, and in this case, the risk is vanishinglya, in that case it is quite difficult to justify thinking about it as much as people currently are. Small research institutes are unlikely to have an impact. Progress in a i capabilities and alignment has been largely driven by empirical work done by small groups of individuals working at highly resourced in compute money and data access organ ations like open a i, d mind, google, et (
<a href=https://share.snipd.com/snip/b73c020e-8cb4-4d96-a64a-6b93503c7051 rel=noopener>Time¬†0:34:09</a>)</li><li>Is There a Funding Constraint to Aligning a i?
Summary:
I worry that people have gotten so focused on the idea that a i risk is so important and dwarfs everything else. People might say, anything is a good investment of money because the problem is so big. Why don&rsquo;t we just ban a i development? I&rsquo;ve never really in answer to the question why are we not talking about a total ban on a i development?"
Transcript:
Speaker 1
Individuals or organizations outside of those certain few organizations are typically not able to contribute meaningfully to alignment as a result, and should not receive critical funds that should instead be spent on influencing these groups. I don&rsquo;t thinkte small research institutes well find anything very important in the first place, because they don&rsquo;t have direct access to manipulate and explore the very models that are being built by these organizations, which are the only models, in my mind, that are pushing a i capabilities forward, and b have any real likelihood of becoming a g i. Eventually, some projects have dubious benefits. One of the issues i got into e a through was global poverty stuff, like probably people did more five years ago than they do now. If someone wants to research a i risk, i&rsquo;m not going to stop them. But then people say, we&rsquo;re going to offer one thousands of dollars of prizes for imagining how hypothetical a i world might look like, because, in theory, people might be inspired by that. The causal chain is tenuous at best. I look at that and say, how many children you could save with that money? It is sort of true that e a isn&rsquo;t funding constraint, but there are clearly things that you could do if you wanted to just take that money and save as many children&rsquo;s lives as possible. I worry that people have gotten so focused on the idea that a i risk is so important and dwarfs everything else, that they throw money at anything that is vaguely connected to the problem. People might say, anything is a good investment of money because the problem is so big. Why don&rsquo;t we just ban a i development? I have been critical about the a i safety discourse because i have never really in answer to the question, if we consider the a i risk to be such a meaningful thing and such a large event, then why are we not talking about a total ban on a i (
<a href=https://share.snipd.com/snip/cc480f7a-cae3-48eb-8d09-4354b8fc3815 rel=noopener>Time¬†0:36:02</a>)</li><li>E A Might Not Be The Best Platform For Investing Great Resources
Summary:
If the field is in a state where they aren&rsquo;t even getting to the nitty gritty, for example, a set of optimization problems or equations, then they should publicize that. The community is technically literate, so it would be worth exposing us to more of the nittigritty details. Investing great resources requires a great justification. I was struck by the trajectory of open philanthropic grant making allocation - global health interventions led the pack for a number of years. Now, a i safety is a close second that warrants a substantial justification in proportion to its resource outlay.
Transcript:
Speaker 1
And if the field is in a state where they aren&rsquo;t even getting to the nitty gritty, for example, a set of optimization problems or equations, then they should publicize that. I realize this is a bit of a weird request, like hay, educate me. But when i give to givewell, i can read their reports. There has been years of soft questioning and pushback by a lot of people in the community. If there was is tran case to be made by researchers about the concrete effectiveness of their projects, i hope it would be made in a more clear and a more public way. I don&rsquo;t want to read short stories by aliza kowski, any more. Short stories by a are great for pulling in casuals on line, but at this point we should have some clear idea of what problems be, they hard or soft. The community is reckoning with the e. A community is technically literate, so it would be worth exposing us to more of the nittigritty details. Investing great resources requires a great justification. I was struck by the trajectory of open philanthropic grant making allocation, where global health interventions led the pack for a number of years. Now, a i safety is a close second that warrants a substantial justification in proportion to its resource outlay. E a might not be the best platform for this. One of my criticisms of the a movement is that i&rsquo;m not sure it&rsquo;s the best platform for promoting investment in a i research. When you have a charitable movement, and then you have people who are active in a i research saying that we need to put mounds of money into a i research, which happens to be a field that gives them their jobs and benefits them personally, then many people are going to become very suspicious of this. Is this just an attempt for all of these people to defraud us so they can live large. (
<a href=https://share.snipd.com/snip/ec68ba5e-3963-4234-bf82-5393983256f8 rel=noopener>Time¬†0:38:14</a>)</li><li>E A Might Not Be The Best Platform For Investing Great Resources
Summary:
E a community is technically literate, so it would be worth exposing us to more of the nittigritty details. Investing great resources requires a great justification. E a might not be the best platform for this. Long time lines mean we should invest less. Many people pretend that a i safety time lines don&rsquo;t matter. It&rsquo;s been good to meet people who are in e a, but not part of the a i community. This makes me more positive about the e a movement.
Transcript:
Speaker 1
Short stories by a are great for pulling in casuals on line, but at this point we should have some clear idea of what problems be, they hard or soft. The community is reckoning with the e. A community is technically literate, so it would be worth exposing us to more of the nittigritty details. Investing great resources requires a great justification. I was struck by the trajectory of open philanthropic grant making allocation, where global health interventions led the pack for a number of years. Now, a i safety is a close second that warrants a substantial justification in proportion to its resource outlay. E a might not be the best platform for this. One of my criticisms of the a movement is that i&rsquo;m not sure it&rsquo;s the best platform for promoting investment in a i research. When you have a charitable movement, and then you have people who are active in a i research saying that we need to put mounds of money into a i research, which happens to be a field that gives them their jobs and benefits them personally, then many people are going to become very suspicious of this. Is this just an attempt for all of these people to defraud us so they can live large. This is one of the reasons why i have been suspicious about the e a movement. But it&rsquo;s been good to meet people who are in e a, but not part of the a i community, people who don&rsquo;t benefit from a i research, makes me more positive about the e a movement in these terms. I would also say that this would be point why the a i research thing should be discussed on many other platforms, chiefly on the political and scientific fields. But the e a community may not be the best platform for this topic. Long time lines mean we should invest less. Many people pretend that a i safety time lines don&rsquo;t matter, as if long term time lines present the same problems a short time lines. (
<a href=https://share.snipd.com/snip/7e373c57-03ec-482f-9af7-8ca3cb4516d2 rel=noopener>Time¬†0:38:45</a>)</li><li>Long Time Lines Mean We Should Invest Less
Summary:
Many people pretend that a i safety time lines don&rsquo;t matter, as if long term time lines present the same problems. This is one of the reasons why i have been suspicious about the e a movement. But it&rsquo;s been good to meet people who are in e a, but not part of the a i community. We should spend less resources than on nuclear war and bio weapons since it might be a very long time until we develop even human level a g i. It might be a mistake to invest too many resources.
Transcript:
Speaker 1
This is one of the reasons why i have been suspicious about the e a movement. But it&rsquo;s been good to meet people who are in e a, but not part of the a i community, people who don&rsquo;t benefit from a i research, makes me more positive about the e a movement in these terms. I would also say that this would be point why the a i research thing should be discussed on many other platforms, chiefly on the political and scientific fields. But the e a community may not be the best platform for this topic. Long time lines mean we should invest less. Many people pretend that a i safety time lines don&rsquo;t matter, as if long term time lines present the same problems a short time lines. They say, a i is coming in 30 to 60 years, an unspoken assumption. But when you push them and say, we will be able to fix the safety problems because it will not come that soon, they say, know, the argument still stand even if you assume long time lines. My problem with the a i safety argument is that, if a g i is coming in ifty to 60 years, short time lines, investing in a i safety research makes a lot of sense if time lines are longer. We should still put an effort, but not that much. In the short term, more nuclear weapons and bio weapons. I don&rsquo;t think a g i is coming in the next 50 years. Beyond that, i&rsquo;m not sure. There been talk that current m l models, when scaled up, will get us to a g i. I don&rsquo;t believe that. Therefore, we should spend less resources than on nuclear war and bio weapons, since it might be a very long time until we develop even human level a g i. It might be a mistake to invest too many resources. Now, interviewer, what if it would come in 20 years? (
<a href=https://share.snipd.com/snip/c72a5eb3-6821-49ce-9e65-add5c3e2641a rel=noopener>Time¬†0:39:44</a>)</li><li>Should We Spend Too Much Resources?
Summary:
Many people pretend that a i safety time lines don&rsquo;t matter, as if long term time lines present the same problems. I am pretty sure about long time lines because of past failures of predictions and because current m l models might be very far away from real intelligence. Long time lines mean we should invest less. We should still put an effort, but not that much. In the short term, more nuclear weapons and bio weapons.
Transcript:
Speaker 1
Long time lines mean we should invest less. Many people pretend that a i safety time lines don&rsquo;t matter, as if long term time lines present the same problems a short time lines. They say, a i is coming in 30 to 60 years, an unspoken assumption. But when you push them and say, we will be able to fix the safety problems because it will not come that soon, they say, know, the argument still stand even if you assume long time lines. My problem with the a i safety argument is that, if a g i is coming in ifty to 60 years, short time lines, investing in a i safety research makes a lot of sense if time lines are longer. We should still put an effort, but not that much. In the short term, more nuclear weapons and bio weapons. I don&rsquo;t think a g i is coming in the next 50 years. Beyond that, i&rsquo;m not sure. There been talk that current m l models, when scaled up, will get us to a g i. I don&rsquo;t believe that. Therefore, we should spend less resources than on nuclear war and bio weapons, since it might be a very long time until we develop even human level a g i. It might be a mistake to invest too many resources. Now, interviewer, what if it would come in 20 years? If i actually was convinced that the time line is like 20 years, then i would definitely say, let&rsquo;s invest a lot of effort, even if it&rsquo;s not likely to succeed. I am pretty sure about long time lines because of past failures of predictions, and because current m l models might be very far away from real intelligence. And i don&rsquo;t these narrative, speculative explanations that current m l models would get there. You cannot reliably predict that, since it is not physics where you can reliably calculate things. (
<a href=https://share.snipd.com/snip/b9ad1859-a64f-4ced-8afa-8d9220151219 rel=noopener>Time¬†0:40:09</a>)</li><li>How to Reduce a i Risk
Summary:
I am pretty sure about long time lines because of past failures of predictions, and because current m l models might be very far away from real intelligence. You cannot reliably predict that, since it is not physics where you can reliably calculate things. We still have to prioritize. I believe there are quite many existential risks. It is important to make sure when we give attention to a i we should not ignore other risks that have the potential of shaping our civilization in the wrong direction.
Transcript:
Speaker 1
I am pretty sure about long time lines because of past failures of predictions, and because current m l models might be very far away from real intelligence. And i don&rsquo;t these narrative, speculative explanations that current m l models would get there. You cannot reliably predict that, since it is not physics where you can reliably calculate things. We still have to prioritize. I believe there are quite many existential risks. Then, of course, you get to the point where you only have a limited amount of resources. And it looks like over the next year&rsquo;s the resources be more and more limited, considering we are on the brink of an economic crisis. So when you have limited amount of resources, even if we theoretically think it&rsquo;s worth spending enormous amount of resources to solve a safety then we still have to do the trade offs with other risks at some point. I think it has been exaggerated. It is important to me to make sure when we give attention to a i we should not ignore other risks that have the potential of shaping our civilization in the wrong direction. We need a better theory of change. I haven&rsquo;t seen a clear theory of change for how this kind of work would actually reduce a i risk. My prior from work in development is that nice sounding ideas without empirical backing tend to fail. Often tend to make the problem worse. So without stronger evidence that x risk from a i is likely an b, working on a i risk can actually mitigate this problem, i am very strongly opposed to its status as a flagship e a cause rogue researchers. We cannot meaningfully reduce a g i risk because of rogue researchers. Dyou a human has been cloned, how do you know if a rogue researcher decides to do it outside the bounds of current (
<a href=https://share.snipd.com/snip/59e8652e-6f2f-41fa-ba57-083b13d2278e rel=noopener>Time¬†0:41:24</a>)</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3></h3><ul class=backlinks><li></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3></h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://braden-w.github.io/quartz/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p></p><ul><li><a href=https://braden-w.github.io/quartz/></a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/braden-w>Github</a></li></ul></footer></div></div></body></html>