<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="[[Deep Learning]] For NLP  Lots of progress recently  People trust [[Wikipedia]], [[Google Translate]], etc. more than other people, in some cases   Language models assign probability to text $p(x_0, ‚Ä¶, x_n)$ Most popular method is to factorize distribution using chain rule: $$p(x_0,‚Ä¶x_n) = p(x_0)p(x_1 \mid x_0) \cdots p(x_n \mid x_{n-1})$$  Neural Language Models [[Convolutional Language Models]] (CLMs)    Earliest"><title>[[Deep Learning]] For NLP</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://braden-w.github.io/quartz//icon.png><link href=https://braden-w.github.io/quartz/styles.c605bbadbdae7a3dff05f2f1a934772a.min.css rel=stylesheet><link href=https://braden-w.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://braden-w.github.io/quartz/js/darkmode.88ed88da9a3165d431ef1df4e618aad8.min.js></script>
<script src=https://braden-w.github.io/quartz/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://braden-w.github.io/quartz/js/popover.53ad9a087e3feeaaa12b63bfd02d923b.min.js></script>
<script src=https://braden-w.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://braden-w.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script>const BASE_URL="https://braden-w.github.io/quartz/",fetchData=Promise.all([fetch("https://braden-w.github.io/quartz/indices/linkIndex.e680114dd38aa7d9c0fd547851b90658.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://braden-w.github.io/quartz/indices/contentIndex.43d89575e3ce07ec985476cd35e7f14b.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://braden-w.github.io/quartz",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://braden-w.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!0,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!0,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'‚Äô':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/braden-w.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label placeholder><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://braden-w.github.io/quartz/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://braden-w.github.io/quartz/>üßô‚Äç‚ôÇÔ∏èOptim</a></h1><div class=spacer></div><div id=search-icon><p></p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title"/><desc id="desc"/><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title/><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title/><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>[[Deep Learning]] For NLP</h1><p class=meta>Jul 30, 2022
<a href=https://github.com/braden-w/quartz/tree/hugo/content/obsidian/Cards/Fleeting%20Notes/Deep%20Learning%20For%20NLP.md rel=noopener></a></p><ul class=tags><li><a href=https://braden-w.github.io/quartz/tags/On/Machine_Learning/Deep_Learning/>On machine learning deep learning</a></li><li><a href=https://braden-w.github.io/quartz/tags/On/Language/>On language</a></li><li><a href=https://braden-w.github.io/quartz/tags/Type/Source/Lecture/>Type source lecture</a></li></ul><aside class=mainTOC><details><summary></summary><nav id=TableOfContents><ol><li><a href=#neural-language-models>Neural Language Models</a><ol><li><a href=#convolutional-language-models-clms>[[Convolutional Language Models]] (CLMs)</a></li><li><a href=#recurrent-language-models>Recurrent Language Models</a></li><li><a href=#attention-is-all-you-needtransformer-language-models>[[Attention Is All You Need|Transformer]] Language Models</a></li></ol></li><li><a href=#decoding-language-models>Decoding Language Models</a><ol><li><a href=#greedy-decoding>Greedy Decoding</a></li><li><a href=#exhaustive-search>Exhaustive Search</a></li><li><a href=#beam-search>Beam Search</a></li><li><a href=#sampling>Sampling</a></li><li><a href=#top-k-sampling>Top-K Sampling</a></li></ol></li><li><a href=#evaluating-text-generation>Evaluating Text Generation</a><ol><li><a href=#backtranslation>[[Backtranslation]]</a></li></ol></li><li><a href=#unsupervised-learning-for-nlp>Unsupervised Learning for NLP</a><ol><li><a href=#word2vec>Word2Vec</a></li><li><a href=#gpt-3gpt>[[GPT-3|GPT]]</a></li><li><a href=#elmo>ELMo</a></li><li><a href=#bert>BERT</a></li></ol></li><li><a href=#examples-of-self-supervised-pretraining-approaches>Examples of Self Supervised Pretraining Approaches</a></li><li><a href=#summary>Summary</a></li></ol></nav></details></aside><a href=#deep-learning-for-nlp><h1 id=deep-learning-for-nlp><span class=hanchor arialabel=Anchor># </span><a href=/quartz/obsidian/Cards/Fleeting-Notes/Deep-Learning rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Deep-Learning>Deep Learning</a> For NLP</h1></a><ul><li>Lots of progress recently<ul><li>People trust <a class="internal-link broken">Wikipedia</a>, <a class="internal-link broken">Google Translate</a>, etc. more than other people, in some cases</li></ul></li><li>Language models assign probability to text $p(x_0, ‚Ä¶, x_n)$</li><li>Most popular method is to factorize distribution using chain rule:
$$p(x_0,‚Ä¶x_n) = p(x_0)p(x_1 \mid x_0) \cdots p(x_n \mid x_{n-1})$$</li></ul><a href=#neural-language-models><h2 id=neural-language-models><span class=hanchor arialabel=Anchor># </span>Neural Language Models</h2></a><a href=#convolutional-language-models-clms><h3 id=convolutional-language-models-clms><span class=hanchor arialabel=Anchor># </span><a href=/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Language-Models rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Language-Models>Convolutional Language Models</a> (CLMs)</h3></a><p><img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-1/fig2.jpg width=auto alt></p><ul><li><p>Earliest</p></li><li><p>Strengths</p><ul><li>Able to learn local dependencies in text data. This is because CLMs use convolutional filters to extract local information from text data.</li></ul></li><li><p>Weaknesses</p><ul><li>They may not be able to learn long-term dependencies in text data. This is because the convolutional filters used by CLMs may not be able to capture long-range dependencies.</li></ul></li></ul><a href=#recurrent-language-models><h3 id=recurrent-language-models><span class=hanchor arialabel=Anchor># </span>Recurrent Language Models</h3></a><p><img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-1/fig3.jpg width=auto alt></p><ul><li>Used to be very popular<ul><li>Conceptually straightforward: every time step we maintain some state (received from the previous time step), which represents what we‚Äôve read so far. This is combined with current word being read and used at later state. Then we repeat this process for as many time steps as we need.</li></ul></li><li>Strengths</li><li>Weaknesses<ul><li><a href=/quartz/obsidian/Cards/Fleeting-Notes/Vanishing-Gradient-Problem rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Vanishing-Gradient-Problem>Vanishing Gradient Problem</a> with long contexts</li><li>The whole history of the document reading is compressed into fixed-size vector at each time step (bottleneck)</li><li>Not possible to parallelize over time-steps, so slow training</li></ul></li></ul><a href=#attention-is-all-you-needtransformer-language-models><h3 id=attention-is-all-you-needtransformer-language-models><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Transformer</a> Language Models</h3></a><p><img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-1/fig4.jpg width=auto alt=300></p><ul><li>Three main stages<ul><li>Input stage</li><li>$n$¬†times transformer blocks (encoding layers) with different parameters</li><li>Output stage</li></ul></li><li>Latest and greatest, revolutionized penalty</li><li>Uses <a href=/quartz/obsidian/Cards/Fleeting-Notes/Multi-Head-Attention rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Multi-Head-Attention>Multi-Head Attention</a> and <a href=/quartz/obsidian/Cards/Fleeting-Notes/Layer-Normalization rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Layer-Normalization>Layer Normalization</a><ul><li>What&rsquo;s the benefit of <a href=/quartz/obsidian/Cards/Fleeting-Notes/Multi-Head-Attention rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Multi-Head-Attention>Multi-Head Attention</a>?<ul><li>To predict the next word you need to observe multiple separate things, in other words attention can be placed on multiple previous words in trying to understand the context necessary to predict the next word.
<img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-1/fig6.png width=auto alt=300></li></ul></li></ul></li><li>Strengths<ul><li><a class="internal-link broken">Transformers</a> share <a href=/quartz/obsidian/Cards/Fleeting-Notes/Weight-W rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Weight-W>Weights</a> across timestamps, unlike Recurrent Language Models</li><li>Very parallelizable</li><li>Scale well<ol><li>Unlimited training data, even far more than you need</li><li>GPT 2 used 2 billion parameters in 2019</li><li>Recent models use up to 17B parameters in 2020</li></ol></li></ul></li><li>Tricks<ul><li>Extensive use of layer normalization to stabilize training</li><li>Warmup + Inverse-square root training schedule</li><li>Careful initialization</li><li>Label smoothing</li></ul></li><li>More Questions<ol><li>How do transformers solve the informational bottlenecks of <a href=/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Neural-Networks rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Convolutional-Neural-Networks>CNNs</a> and RNNs?<ul><li>Attention models allow for direct connection between all words allowing for each word to be conditioned on all previous words, effectively removing this bottleneck.</li></ul></li><li>How do transformers differ from RNNs in the way they exploit GPU parallelization?<ul><li>The multi-headed attention modules in transformers are highly parallelisable whereas RNNs are not and therefore cannot take advantage of GPU technology. In fact transformers compute all time steps at once in single forward pass.</li></ul></li></ol></li></ul><a href=#some-important-facts-of-transformer-language-models><h4 id=some-important-facts-of-transformer-language-models><span class=hanchor arialabel=Anchor># </span>Some Important Facts of Transformer Language Models</h4></a><p><img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-1/fig9.png width=auto alt=300></p><ul><li>Minimal inductive bias</li><li>All words directly connected, which will mitigate vanishing gradients.</li><li>All time-steps computed in parallel.</li><li>Self attention is quadratic (all time-steps can attend to all others), limiting maximum sequence length.</li><li>As self attention is quadratic, its expense grows linearly in practice, which could cause a problem.</li></ul><a href=#decoding-language-models><h2 id=decoding-language-models><span class=hanchor arialabel=Anchor># </span>Decoding Language Models</h2></a><ul><li>So what if we have a probability distribution over text?<ul><li>There are still exponentially many possible outputs, and it will be difficult to compute max</li></ul></li></ul><a href=#greedy-decoding><h3 id=greedy-decoding><span class=hanchor arialabel=Anchor># </span>Greedy Decoding</h3></a><ul><li>Take most likely word at each timestep</li><li>No guarantee that the whole sequence</li></ul><a href=#exhaustive-search><h3 id=exhaustive-search><span class=hanchor arialabel=Anchor># </span>Exhaustive Search</h3></a><ul><li>Suboptimal and often impossible</li></ul><a href=#beam-search><h3 id=beam-search><span class=hanchor arialabel=Anchor># </span>Beam Search</h3></a><ul><li><p>Compromise between <a class="internal-link broken">Greedy Decoding</a> and <a class="internal-link broken">Exhaustive Search</a>
<img src=https://i.imgur.com/UB98BYw.png width=auto alt=300>
<img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-2/Beam_Decoding.png width=auto alt=300></p></li><li><p>How deep does the beam tree branch out ?</p><ul><li>The beam tree continues until it reaches the end of sentence token. Upon outputting the end of sentence token, the hypothesis is finished.</li></ul></li></ul><a href=#sampling><h3 id=sampling><span class=hanchor arialabel=Anchor># </span>Sampling</h3></a><ul><li>Sometimes, we do not want the most likely sequence, but a sample from the model distribution</li><li>Problems<ul><li>Once a ‚Äúbad‚Äù choice is sampled, the model is in a state it never faced during training, increasing the likelihood of continued ‚Äúbad‚Äù evaluation.</li><li>The algorithm can therefore get stuck in horrible feedback loops.</li></ul></li></ul><a href=#top-k-sampling><h3 id=top-k-sampling><span class=hanchor arialabel=Anchor># </span>Top-K Sampling</h3></a><ul><li>A pure sampling technique where you truncate the distribution to the¬†$k$¬†best and then renormalise and sample from the distribution.
<img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-2/Top_K_Sampling.png width=auto alt></li></ul><a href=#question-why-does-top-k-sampling-work-so-well><h4 id=question-why-does-top-k-sampling-work-so-well><span class=hanchor arialabel=Anchor># </span>Question: Why Does Top-K Sampling Work so Well?</h4></a><p>This technique works well because it essentially tries to prevent falling off of the manifold of good language when we sample something bad by only using the head of the distribution and chopping off the tail.</p><a href=#evaluating-text-generation><h2 id=evaluating-text-generation><span class=hanchor arialabel=Anchor># </span>Evaluating Text Generation</h2></a><ul><li><a href=/quartz/obsidian/Cards/Fleeting-Notes/Evaluating-Language-Models-is-Easy-but-Evaluating-Generated-Text-is-Hard rel=noopener class=internal-link data-src=/quartz/obsidian/Cards/Fleeting-Notes/Evaluating-Language-Models-is-Easy-but-Evaluating-Generated-Text-is-Hard>Evaluating Language Models is Easy, but Evaluating Generated Text is Hard</a></li><li>Evaluating language models is easy<ul><li>Just measure likelihood of held out data</li></ul></li><li>Evaluating generated text is hard<ul><li>Trade-off between quality and diversity</li><li>Word overlap metrics with a reference (BLEU, ROUGE etc.) are obviously wrong, but surprisingly useful</li><li>New automated metrics have been proposed</li></ul></li></ul><a href=#backtranslation><h3 id=backtranslation><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Backtranslation</a></h3></a><ul><li>Example: we want to train a German to English translation model</li><li>First, train an English to German reverse translation model using bitext</li><li>Then, translate billions of words of monolingual English to German</li><li>Finally, train German to English model using &lsquo;back-translated&rsquo; data</li></ul><a href=#unsupervised-learning-for-nlp><h2 id=unsupervised-learning-for-nlp><span class=hanchor arialabel=Anchor># </span>Unsupervised Learning for NLP</h2></a><ul><li>How can we learn just from lots of unlabeled text?</li></ul><a href=#word2vec><h3 id=word2vec><span class=hanchor arialabel=Anchor># </span>Word2Vec</h3></a><p>Word embeddings hold some structure
<img src=https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-2/embeddings-structure.png width=auto alt=300></p><a href=#gpt-3gpt><h3 id=gpt-3gpt><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">GPT</a></h3></a><p>Train a conditional language model. Then given this language model, which predicts a word at every time step, replace each output of model with some other feature.</p><ol><li>Pretraining - predict next word</li><li>Fine-tuning - change to a specific task. Examples:<ul><li>Predict whether noun or adjective</li><li>Given some text comprising an Amazon review, predict the sentiment score for the review</li></ul></li></ol><p>Strengths</p><ul><li>We can reuse the model</li><li>Pretrain one large model and fine tune to other tasks</li></ul><a href=#elmo><h3 id=elmo><span class=hanchor arialabel=Anchor># </span>ELMo</h3></a><p><a class="internal-link broken">GPT</a> only considers leftward context, which means the model can‚Äôt depend on any future words - this limits what the model can do quite a lot.</p><p>Here the approach is to train¬†<em>two</em>¬†language models</p><ol><li>One on the text left to right</li><li>One on the text right to left</li><li>Concatenate the output of the two models in order to get the word representation. Now can condition on both the rightward and leftward context.
This is still a ‚Äúshallow‚Äù combination, and we want some more complex interaction between the left and right context.</li></ol><a href=#bert><h3 id=bert><span class=hanchor arialabel=Anchor># </span>BERT</h3></a><p><img src=https://i.imgur.com/JJyKV2A.png width=auto alt></p><p>BERT is similar to word2vec in the sense that we also have a fill-in-a-blank task. However, in word2vec we had linear projections, while in BERT there is a large transformer that is able to look at more context. To train, we mask 15% of the tokens and try to predict the blank.</p><p>Can scale up BERT (RoBERTa):</p><ul><li>Simplify BERT pre-training objective</li><li>Scale up the batch size</li><li>Train on large amounts of GPUs</li><li>Train on even more text</li></ul><p>Even larger improvements on top of BERT performance - on question answering task performance is superhuman now.</p><a href=#examples-of-self-supervised-pretraining-approaches><h2 id=examples-of-self-supervised-pretraining-approaches><span class=hanchor arialabel=Anchor># </span>Examples of Self Supervised Pretraining Approaches</h2></a><ul><li>XLNet:<ul><li>Instead of predicting all the masked tokens conditionally independently, XLNet predicts masked tokens auto-regressively in random order</li></ul></li><li>SpanBERT<ul><li>Mask spans (sequence of consecutive words) instead of tokens</li></ul></li><li>ELECTRA:<ul><li>Rather than masking words we substitute tokens with similar ones. Then we solve a binary classification problem by trying to predict whether the tokens have been substituted or not.</li></ul></li><li>ALBERT:<ul><li>A Lite Bert: We modify BERT and make it lighter by tying the weights across layers. This reduces the parameters of the model and the computations involved. Interestingly, the authors of ALBERT did not have to compromise much on accuracy.</li></ul></li><li>XLM:<ul><li>Multilingual BERT: Instead of feeding such English text, we feed in text from multiple languages. As expected, it learned cross lingual connections better.</li></ul></li><li>The key takeaways from above models<ul><li>Lot of different pre-training objectives work well!</li><li>Crucial to model deep, bidirectional interactions between words</li><li>Large gains from scaling up pre-training, with no clear limits yet</li></ul></li></ul><a href=#summary><h2 id=summary><span class=hanchor arialabel=Anchor># </span>Summary</h2></a><blockquote><p>[!important]
Training models on lots of data beats explicitly modelling linguistic structure. See <a class="internal-link broken">The Bitter Lesson</a>.</p></blockquote><ul><li><p>Transformers are low bias (very expressive) models</p><ul><li>Feeding them lots of text is better than explicitly modelling linguistic structure (high bias)</li></ul></li><li><p>Models can learn a lot about language from predicting in unlabelled text.</p><ul><li>This turns out to be a great unsupervised learning objective</li><li>Fine tuning for specific tasks is then easy</li></ul></li><li><p>Bidirectional context is crucial</p></li></ul><a href=#references><h1 id=references><span class=hanchor arialabel=Anchor># </span>References</h1></a><ul><li>(References::
<a href=https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/ rel=noopener>Deep Learning for NLP ¬∑ Deep Learning</a>)</li><li>(References::
<a href=https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-2/ rel=noopener>Decoding Language Models ¬∑ Deep Learning</a>)</li><li>(References::
<a href="https://www.youtube.com/watch?v=6D4EWKJgNn0" rel=noopener>Week 12 ‚Äì Lecture: Deep Learning for Natural Language Processing (NLP) - YouTube</a>)</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3></h3><ul class=backlinks><li></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3></h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://braden-w.github.io/quartz/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p></p><ul><li><a href=https://braden-w.github.io/quartz/></a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/braden-w>Github</a></li></ul></footer></div></div></body></html>