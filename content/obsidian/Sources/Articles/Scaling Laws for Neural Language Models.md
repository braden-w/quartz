---
tags:
  - On/Language
  - On/Artificial_Intelligence
  - Type/Source/Paper
title: Scaling Laws for Neural Language Models
date: "2022-06-29"
date modified: "2022-06-29"
---

# Scaling Laws for Neural Language Models
Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.

# References
- (References:: [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361))
