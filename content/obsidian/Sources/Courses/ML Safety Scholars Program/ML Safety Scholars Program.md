---
date: "2022-07-21"
date modified: "2022-09-12"
draft: false
tags:
- On/Artificial_Intelligence
- On/Machine_Learning
- Type/Source/Course
title: ML Safety Scholars Program
---

# ML Safety Scholars Program
[[Introduction to Machine Learning (MIT)]]
[[Deep Learning for Computer Vision (UMich)]]
[[Week 2 Lab]]

## [[2022-06-22]] Week 1 Readings
[[We Need to Talk About A.I.]]

## [[2022-06-29]] Week 3 Readings
[[The Bitter Lesson]]
[[Dropout - A Simple Way to Prevent Neural Networks From Overfitting]]
[[Adam - A Method for Stochastic Optimization  Abstract]]

## [[2022-07-18]] Week 4 Papers
[[Layer Normalization]]
[[Deep Residual Learning for Image Recognition]]

## [[2022-07-25]] Week 5 Papers
[[BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding - Abstract]]
[[Attention Is All You Need]]
[[Unsolved Problems in ML Safety]]

[[Estimated Computation Used in Large AI Training Runs]]

[[2022-07-19 ML Safety Scholars Program Call About Philosophy, Applicability, Composition]]

## [[2022-07-26]] Week 5 Reading
- [[Advanced, Planning, Strategically Aware Systems]]
- [[Narrow AI]] and [[P=NP Problem]]
- [[Disjunctive Reasoning]] versus [[Conjunctive]] Reasoning
- [[Failure Mode]]
- [Is Power-Seeking AI an Existential Risk?](https://www.josephcarlsmith.com/_files/ugd/5f37c1_5333aa0b7ff7461abc208b25bfc7df87.pdf)

- It's interesting how a lot of methods have been based off [[Nature]]
	- Consider [[Dropout - A Simple Way to Prevent Neural Networks From Overfitting|Dropout]], which is based off [[Evolution]]
- However, I then think of the [[Iterative Approach (of Evolution)]]. We may be missing out on [[Intelligent Design]] since nature focuses on what worked, not what could be.
	- See [[The Bitter Lesson]]
[[Intelligent Design]]

[[Deep Learning For NLP]]

[[Intro to ML Safety]]

## [[2022-08-03]] Week 7 Reading
- [[Alignment]] and [[Honest AI]]
- [[Trojan Horse Models]]
- [[Honesty]] versus [[Truthfulness]]
	- Honesty is trying to tell the truth, whereas Truthfulness is actually being true
- The reading says, “Philosophical/fuzzy reasoning ability and raw intelligence seem distinct; by default high-IQ educated people are not especially good at reasoning about fuzzy abstract objects.” Debate this
	- [[Flynn Effect]]

> Some studies have found a reverse Flynn effect with declining scores for those with high IQ. In 1987, Flynn took the position that **the very large increase indicates that IQ tests do not measure intelligence but only a minor sort of "abstract problem-solving ability" with little practical significance**.

## Resources
[GitHub MIT Course Notes](https://phillipi.github.io/6.882/2020/notes/6.036_notes.pdf)
[Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap1.html)
[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/)>)
