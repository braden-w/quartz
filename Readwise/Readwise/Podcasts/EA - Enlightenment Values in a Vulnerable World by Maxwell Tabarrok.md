# EA - Enlightenment Values in a Vulnerable World by Maxwell Tabarrok

![rw-book-cover](https://images.weserv.nl/?url=https%3A%2F%2Fspeechkit-prod.s3.eu-west-1.amazonaws.com%2Fdistribution_images%252Fdistribution%252F%252FNonlinearnormal.png&w=100&h=100)

## Metadata
- Author: [[The Nonlinear Library]]
- Full Title: EA - Enlightenment Values in a Vulnerable World by Maxwell Tabarrok
- Category: #podcasts
- URL: https://share.snipd.com/episode/d9c08fd7-5061-4837-a37a-682d6208c7de

## Highlights
- The Urn Model of Uncertainty
  Transcript:
  Speaker 1
  Complete randomness assumes that we have no knowledge about what the risk of a technology might be before it is actually invented. An important clarification bostrum makes is that we can speak of vulnerabilities opening and closing. In the easy nu reo, the period of vulnerability begins when the easy way of producing nuclear explosions is discovered. It ends when some level of technology is attained that makes it reasonably affordable to stop nuclear explosions from causing unacceptable damage. This implies that the color of balls in the urn, i e, the risk from technologies, is not constant or in pendent of the balls which come before it. If the technology which abates nuclear risks came before easy neks, then that ball would have changed color, and no vulnerability would have opened. Additionally, the metaphor would also become more realistic if we imagine that there is not just one hand daintily exploring the urn. Instead, picture a throng of scuffling prospect sreaching in their arms in hopes of gold and glory and citations wide progress technological anecdotes. For the next two subsections, we'll model the color of pulls from the urn as random, but not independent. That is, some technologies can change the risks of others, but we don't know what the risk of a technology will be before we invent it. Another basic assumption is that technological maturity, i e, the inevitable topping out of our expedential growth into an s curve, is desirable and stable, but the path there may be dangerous. Another way to incode this assumption is if we discovered all possible technologies at once, which, in bostrum's wide definition of technology in the v w h paper, includes ideas about coordination and insight, we would be in the safe region. ([Time 0:04:47](https://share.snipd.com/snip/8d6e2c42-6798-476d-a9bc-9acb35c4559c))
- The Global Surveillance State
  Summary:
  A state which is alined with the interests of some specific religion, race or an even smaller oligarchic group can preside over and perpetrate the killing of billions of people. The history of government gives no evidence that alignment with decreasing global catastrophic risk is stable. Such a state could an antidote, but whether it would do so isn't obvious. Global surveillance states have strong incentives to develop dangerous technologies to guarantee authority over humanity's dangerous technological development.
  Transcript:
  Speaker 1
  If the global state falls into the control of a group with less than global interests, the alignment of the state towards global catastrophic risks will not hold. A state which is alined with the interests of some specific religion, race or an even smaller oligarchic group, can preside over and perpetrate the killing of billions of people and still come out ahead with respect to its narrow interests. The history of government gives no evidence that alignment with decreasing global catastrophic risk is stable. By contrast, there is evidence that alignment with the interests of some powerful subset of constituents is essentially the default condition of government. Bostrum is right that minimizing existential risk requires a stable and powerful global government, then politicide, propaganda, genicide, scapegoading and stagnation are all instrumental in pursuing the strategy of minimizing anthropogenic risk. A global state with this goal is therefore itself a catastrophic risk if it disarmed. Other more dangerous risks. Such a state could an antidote, but whether it would do so isn't obvious. In the next section, we consider whether the ponopticon government is likely to disarm many existential risks. Global surveillance states have strong incentives to develop dangerous technologies to guarantee authority over humanity's dangerous technological development. The global surveillance state will try to keep their technology level as high as possible relative to their constituents. We saw above, one part of their strategy enforcing technological stagnation. This alone may not be sufficient. However, the state may benefit from using technology to increase its capacity for longevity and control. ([Time 0:23:49](https://share.snipd.com/snip/28a4cffb-1d67-4af3-86cc-9f4920c82662))
- Existential Risk
  Transcript:
  Speaker 1
  Bostrum's penopticon government does not look especially promising. An organization or agent who is empowered and motivated to pursue existential risk red non this level will find it necessary to sustain the filter's authority over humanity for centuries to come. Totalitarian strategies of crushing descent, genecide of scapegoats enforce stagnation and the development of world destroying weapons will be useful for securing the power that is a necessary instrument for this pilter. The power which comes with the ability to construct and enforce this filter will be the ultimate prize sought after by anyone with interests narrower than the future of all humanity. Small groups could easily coordinate and use the filter mechanism to spread costly externalities or risks among the rest of the world, while greatly enriching themselves. Even worse, some groups will seek to use the filter mechanism toout right destroy others. There is currently no known institutional design with this kind of power that has demonstrated even temporary immunity to this misallignment. All current states sacrifice the interests of present groups and especially future people, to benefit concentrated interests within their borders. Whether the filter is allined with the interests of humanity as a whole or not, it will hold on to its power ruthlessly. So it represents a significant existential risk. Previous governments include several cases of genecide, which approach bostrum's definition of existential risk. So bigger and more powerful versions of government seem like an unpromising strategy to reduce net risk. Given the difficulty of correctly setting up this filter and the danger of getting it wrong, we ought to look for safer and easier ways to decrease our existential risk. Upholding enlightenment values is a good place to start the search for this optumal risk reduction strategy. ([Time 0:36:25](https://share.snipd.com/snip/9fe7357f-9c7a-46f1-a73f-7ef89088b512))
- Black Ball
  Summary:
  Around 18 hundred there have probably been at least 100 million patents world wide. There are 120 million papers in the microsopt academic data base. We can be confident that these numbers severely undercount the number of inventions and scientific ideas. A reasonable estimate of all the acts of invention and scientific discovery not tracked by these data, plus all the other moramorphous concepts also in the urn, easily exceeds 500 million.
  Transcript:
  Speaker 1
  year. Since around 18 hundred there have probably been at least 100 million patents world wide, which is itself a lower bound for the number of inventions. And there are 120 million papers in the microsopt academic data base. We can be confident that these numbers severely undercount the number of inventions and scientific ideas. And they do not even attempt to capture institutional designs, organizational techniques, ideologies, concepts and means. A reasonable estimate of all the acts of invention and scientific discovery not tracked by these data, plus all the other moramorphous concepts also in the urn, easily exceeds 500 million. Following toby ord's estimations of natural existential risk. We can use this historical data to put plausible upper bounds per draw risk of pulling a black ball. Let's normalize to groups of 100 thousand balls to save space on digits. So we've probably pulled between two thousand, 210 thousand groups of 100 k balls from the urn of knowledge. If we had a 99 % chance of avoiding extinction or catastrophe with each group, there would be at most a point zero, zero, zero, zero, zero, zero, zero, zero, zero, two five % chance of surviving as long as we have. For our history to be more likely than a one and one thousand chance, we'd have to have to have at the very least 99 point six % of surviving each group of 100 k draws without incident. If you think we've drawn more than 220 million balls so far, this minimum probability of safety increases further for our history of no blackball incidents to be more likely than not. We need a probability of safety for each group of 100 k draws between point nine, nine nine seven and point nine nine nine nine three, depending on how many draws you think we've had so far. ([Time 0:48:04](https://share.snipd.com/snip/c2493185-2e9b-4e12-935c-87bbcf6fef72))
- How Many Black Balls Have We Seen So Far?
  Summary:
  A reasonable estimate of all the acts of invention and scientific discovery not tracked by these data, plus all the other moramorphous concepts also in the urn, easily exceeds 500 million. We can use this historical data to put plausible upper bounds per draw risk of pulling a black ball. Let's normalize to groups of 100 thousand balls to save space on digits. So we've probably pulled between two thousand, 210 thousand groups of 100 k balls from the urn of knowledge. If we had a 99 % chance of avoiding extinction or catastrophe with each group, there would be at most a point zero, zero,zero, zero, 0, zero, Zero, zero, zero,. zero,
  Transcript:
  Speaker 1
  And they do not even attempt to capture institutional designs, organizational techniques, ideologies, concepts and means. A reasonable estimate of all the acts of invention and scientific discovery not tracked by these data, plus all the other moramorphous concepts also in the urn, easily exceeds 500 million. Following toby ord's estimations of natural existential risk. We can use this historical data to put plausible upper bounds per draw risk of pulling a black ball. Let's normalize to groups of 100 thousand balls to save space on digits. So we've probably pulled between two thousand, 210 thousand groups of 100 k balls from the urn of knowledge. If we had a 99 % chance of avoiding extinction or catastrophe with each group, there would be at most a point zero, zero, zero, zero, zero, zero, zero, zero, zero, two five % chance of surviving as long as we have. For our history to be more likely than a one and one thousand chance, we'd have to have to have at the very least 99 point six % of surviving each group of 100 k draws without incident. If you think we've drawn more than 220 million balls so far, this minimum probability of safety increases further for our history of no blackball incidents to be more likely than not. We need a probability of safety for each group of 100 k draws between point nine, nine nine seven and point nine nine nine nine three, depending on how many draws you think we've had so far. These numbers are credible bounds on the chance of catastrophe from a given invention, if the existential risk per invention is not increasing over time, which may be a suspect assumption. ([Time 0:48:21](https://share.snipd.com/snip/cd16677a-4677-4c72-8a5a-e3e0725ea58c))
- The Probability of Pulling a Black Ball
  Summary:
  If we had a 99 % chance of avoiding extinction or catastrophe with each group, there would be at most a point zero, zero, zero. We've probably pulled between two thousand, 210000 groups of 100 k balls from the urn of knowledge. Even accepting these bounds does not necessarily relieve worries about technological x risk. If our invention rate keeps growing like it is, in 200 years, we might be inventing three billion things a year. That's 30 thousand groups of 100k inventions. And that's just one typical year in two thousand 200.
  Transcript:
  Speaker 1
  So we've probably pulled between two thousand, 210 thousand groups of 100 k balls from the urn of knowledge. If we had a 99 % chance of avoiding extinction or catastrophe with each group, there would be at most a point zero, zero, zero, zero, zero, zero, zero, zero, zero, two five % chance of surviving as long as we have. For our history to be more likely than a one and one thousand chance, we'd have to have to have at the very least 99 point six % of surviving each group of 100 k draws without incident. If you think we've drawn more than 220 million balls so far, this minimum probability of safety increases further for our history of no blackball incidents to be more likely than not. We need a probability of safety for each group of 100 k draws between point nine, nine nine seven and point nine nine nine nine three, depending on how many draws you think we've had so far. These numbers are credible bounds on the chance of catastrophe from a given invention, if the existential risk per invention is not increasing over time, which may be a suspect assumption. However, even accepting these bounds does not necessarily relieve worries about technological x risk, despite the microscopic probabilities they place on it. Even if the chance of catastrophe per draw from the urn is not increasing, the number of draws we are taking is increasing. If our invention rate keeps growing like it is, in 200 years, we might be inventing three billion things a year. That's 30 thousand groups of 100 k inventions. Even if each group has a 99 point nine nine three % chance to not kill us, getting 30 thousand of these in a row is around a 12 % chance. And that's just one typical year in two thousand 200. ([Time 0:48:51](https://share.snipd.com/snip/246b7d14-a26f-4008-935f-0bf63f60669b))
